<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="公开课笔记," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="理论知识核（Kernels）我们之前讲线性回归的时候，遇到过这样一个问题，其中输入的特征 $x$ 是一个房屋的居住面积，然后我们考虑使用特征 $x$，$x^2$ 以及 $x^3$ 来进行拟合得到一个立方函数（cubic function）。要区分出两组数据集，我们把原始的输入值称为一个问题的输入属性，例如住房面积 $x$。当这些值映射到另外的一些数据集来传递给学习算法的时候，这些新的数据值就称为输">
<meta name="keywords" content="公开课笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="第八集 顺序最小优化算法">
<meta property="og:url" content="http://zzysay.github.io/2017/08/05/cs229-8/index.html">
<meta property="og:site_name" content="ZZY SAY">
<meta property="og:description" content="理论知识核（Kernels）我们之前讲线性回归的时候，遇到过这样一个问题，其中输入的特征 $x$ 是一个房屋的居住面积，然后我们考虑使用特征 $x$，$x^2$ 以及 $x^3$ 来进行拟合得到一个立方函数（cubic function）。要区分出两组数据集，我们把原始的输入值称为一个问题的输入属性，例如住房面积 $x$。当这些值映射到另外的一些数据集来传递给学习算法的时候，这些新的数据值就称为输">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img040.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img041.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img042.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img043.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img044.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img045.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img046.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img047.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img048.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img049.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img050.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img051.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img052.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img053.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img054.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img055.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img056.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img057.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img058.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img059.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img060.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img061.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img062.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img063.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img064.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img065.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img066.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img067.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img068.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note0801.png">
<meta property="og:updated_time" content="2017-08-05T13:03:52.702Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第八集 顺序最小优化算法">
<meta name="twitter:description" content="理论知识核（Kernels）我们之前讲线性回归的时候，遇到过这样一个问题，其中输入的特征 $x$ 是一个房屋的居住面积，然后我们考虑使用特征 $x$，$x^2$ 以及 $x^3$ 来进行拟合得到一个立方函数（cubic function）。要区分出两组数据集，我们把原始的输入值称为一个问题的输入属性，例如住房面积 $x$。当这些值映射到另外的一些数据集来传递给学习算法的时候，这些新的数据值就称为输">
<meta name="twitter:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note3-img040.jpg?imageMogr2/thumbnail/!75p">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zzysay.github.io/2017/08/05/cs229-8/"/>





  <title>第八集 顺序最小优化算法 | ZZY SAY</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZZY SAY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">I AM WHO I AM.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://zzysay.github.io/2017/08/05/cs229-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG ZH.Y.">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZZY SAY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第八集 顺序最小优化算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-05T20:34:56+08:00">
                2017-08-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/Andrew-Ng公开课/" itemprop="url" rel="index">
                    <span itemprop="name">Andrew Ng公开课</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><h3 id="核（Kernels）"><a href="#核（Kernels）" class="headerlink" title="核（Kernels）"></a>核（Kernels）</h3><p>我们之前讲线性回归的时候，遇到过这样一个问题，其中输入的特征 $x$ 是一个房屋的居住面积，然后我们考虑使用特征 $x$，$x^2$ 以及 $x^3$ 来进行拟合得到一个立方函数（cubic function）。要区分出两组数据集，我们把原始的输入值称为一个问题的输入属性，例如住房面积 $x$。当这些值映射到另外的一些数据集来传递给学习算法的时候，这些新的数据值就称为输入特征。然后 我们还要用 $\phi$ 来表示特征映射，这种特征映射也就是从输入的属性到传递给学习算法的输入特征之间的映射关系。例如这个居住面积 $x$ 的例子，这里面的特征映射就可以表述为： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img040.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<a id="more"></a>
<p>现在我们就不再简单直接地利用 SVM 来处理原始的输入属性 $x$ 了，而是可以尝试着利用映射产生的 新的特征 $\phi(x)$。那么，我们只需要把所有的 $x$ 替换成 $\phi(x)$。  上面的算法可以用 $<x, z="">$ 的内积形式写出，这就意味着我们只需要把上面的所有内积都替换成 $&lt;\phi(x), \phi(z)&gt;$的内积。更简洁地说，给定一个特征映射$\phi(x)$，那么就可以定义一 个对应的核（Kernel），如下所示：</x,></p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img041.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p> 然后，只需要把上面的算法里用到的$<x, z="">$全部替换成 $K(x, z)$就可以了，我们的算法就开始使用特征映射  $\phi$ 来进行机器学习了。 现在，给定一个特征映射  $\phi$，很容易就可以找出 $\phi(x)$ 和 $\phi(z)$ ，然后取内积，就能计算 $K(x,z)$。不过更有意思的是，通常情况下对这个 $K(x,z)$ 的计算往往都会非常容易，甚至即便 $\phi(x)$  本身很不好算的时候（向量维度极高）也如此。在这样的背景下，给定一个特征映射 $\phi$，我们就可以使用 SVM 来对高纬度特征空间进行机器学习，根本不用麻烦地区解出来对应的向量 $\phi(x)$ 。 </x,></p>
<p>假设有 $x,z \in R^n$, 设有：  </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img042.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这个也可以写成下面的形式：  </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img043.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>因此，可见 $K (x,z) = \phi(x)^T \phi(z)$，其中特征映射 $\phi$ 给出如下所示 (n=3) ： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img044.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>到这里就会发现，计算高维度的 $\phi(x)$ 需要的计算量是 $O(n^2)$ 级别的，而计算 $K (x,z)$ 则只需要 $O(n)$ 级的时间，也就是 与输入属性的维度呈线性相关关系。 与之相关的核（Kernel）可以设为： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img045.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>对应的特征映射为 （n=3）： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img046.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>其中 参数 $c$ 控制了第一组 $x_i$ 和第二组 $x_ix_j$ 的相对权重。 在此基础上进一步扩展，核 $K(x,z) = (x^Tz + c)^d$，就对应了一个 <img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img047.jpg?imageMogr2/thumbnail/!75p" alt="">维度的特征空间的特征映射，对应所有从 $x_{i1}x_{i2} …x_{ik}$  一直到 d 这种形式的所有多项式。 然而，即便是针对这种 $O(n^d)$ 维度的高维度空间，计算 $K (x, z)$ 仍然只需要 $O(n)$ 级的时间。所以我们就不需要在这个非常高的维度特征空间中对特征向量进行具体的表示。 现在，我们来从另外一个角度来看一下核。凭直觉来看， 如果 $\phi(x)$ 和$\phi(z)$ 非常接近，那么就可能 会认为 $K(x,z) = \phi(x)^T\phi(z)$ 就可能会很大。与之相反，如果 $\phi(x)$ 和$\phi(z)$ 距离很远，比如近似正交，那么 $K(x,z) = \phi(x)^T\phi(z)$  就可能会很小。这样，我们就可以把核 $K(x,z)$ 理解成对 $\phi(x)$ 和$\phi(z)$ 的近似程度的一种度量手段，或者也可以说是对 $x$ 和 $z$ 的近似程度的一种度量手段。 有了这种直观认识之后，假如你正在尝试某些学习算法，并且 已经建立了某个函数 $K(x,z)$，然后你想着也许可以用这个函数来对 $x$ 和 $z$ 的近似程度进行衡量。例如下面这个函数： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img048.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这个函数是对 $x$ 和 $z$ 的近似程度的一个很好的衡量，二者相近的时候函数值接近 1，而二者远离的时候函数值接近 0。那 咱们能不能用这样定义的一个函数 $K$ 来作为一个 SVM  里面的核呢？在这个特定的样例里面，答案是可以的。（这个核也叫做高斯核，对应的是一个无穷维度的特征映射 $\phi$）那么接下来进一步推广，给定某个函数 $K$，我们该怎样能够确定这个函数是 不是一个有效的核呢？例如，我们能否说如果存在着某一个特征映射 $\phi$，则对于所有的 $x$ 和 $z$ 都有 $K(x,z) = \phi(x)^T\phi(z)$？<br>现在暂时假设 $K$ 就是一个有效的核，对应着某种特征映射 $\phi$。然后，考虑某个有 $m$ 个点的有限集合  $ {x^{(1)},…,x^{(m)}}$，然后设一个方形的 $m×m$ 矩阵 $K$，定义方式为矩阵的第 $(i, j)$ 个值 $K_{ij} = K(x^{(i)} , x^{(j)})$。 这个矩阵就叫做核矩阵（Kernel matrix）。这里对符号 K 进行了重复使用，既指代了 $K(x,z)$ 这个核函数，也指代了核矩阵 $K$。如果 $K$ 是一个有效的核，那么就有 $K_{ij} = K(x^{(i)} , x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})  = \phi(x^{(j)})^T\phi(x^{(i)}) = K_{ji} $，这就说明 $K$ 是一个对称矩阵。此外，设 $\phi_k(x)$ 表示向量 $\phi(x)$ 的第 $k$ 个 坐标值，对于任意的向量 $z$，都有： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img049.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>由于 $z$ 是任意的，这就表明了矩阵 $k$ 是半正定的矩阵。 这样，我们就证明了，如果 $k$ 是一个有效核函数，那么对应的核矩阵 $K \in R^{m×m}$ 就是一个对称正半定矩阵 （symmetric positive semidefinite）。进一步扩展，这就不仅仅 是一个 K 是一个有效核函数的必要条件，还成了充分条件，这个核函数也叫做默瑟核（Mercer kernel）。下面要展示的结果都是源自于 Mercer。 </p>
<p>默瑟定理（Mercer’s theorem）：设给定的 $K: R^n×R^n → R$。使 $K$ 为一个有效的默瑟核的充分必要条件为：对任意的 $ {x^{(1)},…,x^{(m)}}, (m &lt; ∞)$ 都有对应的核矩阵为对称半正定矩阵。<br>对于一个给定的函数 $K$，除了之前的找出对应的特征映射 $\phi$ 之外，上面的定理还给出了另外一种方法来验证这个函数是否为有效核函数。</p>
<p>关于在支持向量机算法中核的使用，我们讲得已经够清楚了，所以就不在这里多做赘述了。不过值得记住的是，核 的用法远远不仅限于 SVM 算法当中。具体来说，只要你的学习算法能够写成仅用输入属性向量的内积来表达 的形式，那么就可以通过引入核，替换成 $K(x,z)$，来对你的算法加速，使之能够在与 $K$ 对应 的高维度特征空间中有效率地运行。核与感知器相结合，还可以产生内核感知器算法（kernel perceptron algorithm）。后文我们要学到的很多算法，也都可以适用于这样的处理，这个方法也就成为核技巧（kernel trick）。</p>
<h3 id="正则化和不可区分的情况（Regularization-And-The-Non-Separable-Case）"><a href="#正则化和不可区分的情况（Regularization-And-The-Non-Separable-Case）" class="headerlink" title="正则化和不可区分的情况（Regularization And The Non-Separable Case）"></a>正则化和不可区分的情况（Regularization And The Non-Separable Case）</h3><p>到目前为止，我们对 SVM 进行的推导都是基于一个假设，也就是所有的数据都是线性可分的。在通过特征映射 $\phi$ 来将数据映射到高维度特征空间的过程，通常会增加数据可分割的概率，但我们还是不能保证数据一直可以区分。而且，在某些案例中，查找一个分类超平面还不一定是我们的目的所在，因为也可能很容易就出现异常值。例如，如下图所示的是一个最优边界分类器（optimal margin classifier），如果有一个单独的异常值投到了右上方的区域，这就会导致分界线出现显著的偏移，还会导致分类器的边界缩小了很多：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img050.jpg?imageMogr2/thumbnail/!75p" alt=""><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img051.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>要想让算法能够适用于非线性可分的数据集，并且降低其对待异常值的敏感度，那就要把我们的优化方法进行重构 （reformulate）（使用 L1 正则化），如下所示： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img052.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这样就允许数据集里面有（函数）边界小于1 的情况了，然后如果一个样本的函数边界为 $1 − ξ_i$ ($ ξ &gt; 0$)，这就需要我们给出 $Cξ_i$ 作为目标函数降低成本。$C$ 是一个参数，用于控制相对权重，具体的控制需要在一对目标之间进行考量，一个是使得 $||w||_2$ 取小值，另一个是确保绝大部分的样本都有至少为 1 的函数边界。</p>
<p>给出拉格朗日函数：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img053.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上面的式子中的 $α_i$ 和 $r_i$ 都是拉格朗日乘数。设关于 $w$ 和 $b$ 的导数为零，然后再代回去进行简化，这样就能够得到下面的该问题的对偶形式：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img054.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>跟以前一样，我们还是可以把 $w$ 用 $α_i$ 来进行表述。这里要注意，就是在加入了 L1 正则化之后，对对偶问题的唯一改变只是约束从原来的 $0 ≤ α_i$ 现在变成了 $0 ≤ α_i ≤ C$。这里对 $b^∗$ 的计算也受到了影响而有所变动， 另外，KKT 对偶互补条件（这个在下一节要用来测试 SMO 算法的收敛性）为： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img055.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>现在，剩下的问题就只是给出一个算法来具体地解决这个对偶问题了。</p>
<h3 id="SMO-优化算法（Sequential-Minimal-Optimization）"><a href="#SMO-优化算法（Sequential-Minimal-Optimization）" class="headerlink" title="SMO 优化算法（Sequential Minimal Optimization）"></a>SMO 优化算法（Sequential Minimal Optimization）</h3><p>对于从 SVM 推导出的对偶问题，SMO 算法提供了一种有效的解法。其中，坐标上升算法（coordinate ascent algorithm）是用来推导出 SMO 优化算法的主要部分。</p>
<h4 id="坐标上升算法（Coordinate-Ascent）"><a href="#坐标上升算法（Coordinate-Ascent）" class="headerlink" title="坐标上升算法（Coordinate Ascent）"></a>坐标上升算法（Coordinate Ascent）</h4><p>假如要解决下面这样的无约束优化问题：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img056.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这里的 $W$ 就是关于参数 $α_i$ 的某种函数，此处暂时忽略掉这个问题和支持向量机算法的任何关系。更早之前我们就已经学过了两种优化算法了，梯度下降法和牛顿法。下面这个新的优化算法，就叫做坐标上升算法（coordinate ascent）： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img057.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>所以如上式中所示，算法内层的循环中，会对除了某些特定的 $α_i$ 之外的所有变量进行保存，然后重新优化 $W$ 来调整参数 $α_i$。这里给出的这个版本的算法，内层循环对变量重新优化的顺序是按照变量排列次序 $α_1, α_2,…, α_m, α_1, α_2,…$进行的， 更复杂的版本可能还会选择其他的排列；例如，我们可以根据预测哪个变量可以使 $W(α)$ 增加最多，来选择下一个更新的变量：</p>
<p>如果在函数 $W$ 中，内层循环中的 “arg max” 可以很有效地运行，那么坐标上升算法就成了一个相当有效率的算法了。下面是一个坐标上升算法的示意图：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img058.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上图中的椭圆形就是我们要优化的二次函数的轮廓线。坐标上升算法的初始值设置为 (2, −2)，此外图中还标示了到全局大值的路径。要注意，坐标上升法的每一步中，移动的方向都是平行于某个坐标轴的，因为每次都只对一个变量进行了优化。 </p>
<h4 id="SMO-优化算法（Sequential-Minimal-Optimization）-1"><a href="#SMO-优化算法（Sequential-Minimal-Optimization）-1" class="headerlink" title="SMO 优化算法（Sequential Minimal Optimization）"></a>SMO 优化算法（Sequential Minimal Optimization）</h4><p>接下来，我们来简单推导一下 SMO 算法，下面就是一个（对偶）优化问题：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img059.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>我们假设有一系列满足约束条件 (18-19) 的 $α_i$ 构成的集合。 接下来，假设我们要保存固定的 $α_2, …, α_m$ 的值，然后进行一步坐标上升，重新优化 $α_1$对应的目标值。这样能解出来么？不能，因为约束条件 (19) 就意味着： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img060.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>或者，也可以对等号两侧同时乘以 $y^{(1)}$ ，然后会得到下面的等式，与上面的等式是等价的： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img061.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>（这一步用到了一个定理，即 $y^{(1)} \in  {−1, 1}$，所以 $ (y^{(1)})^2 = 1 $）<br>可见 $α_1$ 是由其他的 $α_i$ 决定的，这样如果我们保存固定的 $α_2, …, α_m$ 的值，那就根本没办法对 $α_1$ 的值进行任何修改，否则不能满足优化问题中的约束条件 (19) 了。 </p>
<p>所以，如果我们要对 $α_i$ 当中的一些值进行更新的话，就必须至少同时更新两个，这样才能保证满足约束条件。基于这个情况就衍生出了 SMO 算法：<br><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img062.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>重复直到收敛 {</p>
<ol>
<li>选择某一对的  $α_i$ 和 $α_j$  用来在下次迭代中进行更新（选择朝全局最大值方向靠近的一对值）</li>
<li>使用对应的  $α_i$ 和 $α_j$ 来重新优化 $W(α)$ ，保持其他的 $α_k$ 值固定。<br>}  </li>
</ol>
<p>我们可以检查在某些收敛公差参数 $tol$ 范围内，$KKT$ 对偶互补条件能否被满足，以此来检验这个算法的收敛性。这里的 $tol$ 是收敛公差参数（convergence tolerance parameter），通常都是设定到大概 0.01 到 0.001。 SMO 算法有效的一个关键原因是对 $α_i$, $α_j$ 的更新计算很有效率。接下来我们简要介绍一下推导高效率更新的大概思路。 </p>
<p>假设我们现在有某些 $α_i$ 满足约束条件 (18-19)，我们决定要保存固定的 $α_3, …, α_m$ 值，然后使用这组 $α_1$ 和 $α_2$ 来重新优化 $W (α_1, α_2, …, α_m)$ ，根据约束条件 (19)，可以得到： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img063.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>等号右边的值是固定的，因为我们已经固定了 $α_3, …, α_m$ 的值，所以就可以把等号右边的项目简写成一个常数 $ζ$:</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img064.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>我们可以用下面的图来表述对 $α_1$ 和 $α_2$ 的约束条件： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img065.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>根据约束条件(18)，可知 必须在图中 $α_1$ 和 $α_2$ 必须在 $[0, C] × [0, C]$ 所构成的方框中。另外图中还有一条线 $α_1y^{(1)} +α_2y^{(2)} = ζ$，而我们知道 $α_1$ 和 $α_2$ 必须在这条线上。还需要注意的是，通过上面的约束条件，还能知道 $L ≤ α_2 ≤ H$；否则 $(α_1,α_2)$ 就不能同时满足在方框内并位于直线上这两个约束条件。在上面这个例子中，$L = 0$。但考虑到直线$α_1y^{(1)} +α_2y^{(2)} = ζ$ 的形状方向， $L = 0$ 未必就是最佳结果；不过通常来讲，保证 $α_1$, $α_2$ 位于 $[0, C] × [0, C]$ 方框内的 $α_2$ 可能的值， 都会有一个下界 $L$ 和一个上界 $H$。 利用等式 (20)，我们还可以把 $α_1$ 写成 $α_2$ 的函数的形式： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img066.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>（这一步同样用到了 $ (y^{(1)})^2 = 1 $）</p>
<p>所以目标函数 $W(α)$ 就可以写成： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img067.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>把 $α_3, …, α_m$ 当做常量，你就能证明上面这个函数其实只是一个关于 $α_2$ 的二次函数。也就可以写成 $aα_2 + bα_2 + c$ 的形式，其中的 $a, b, c$ 参数。如果我们暂时忽略掉方框约束条件 (18)，那就很容易通过使导数为零来找出此二次函数的最大值，继而进行求解。我们设 $α_2^{new, unclipped}$ 表示为 $α$ 的结果值。如果我们要使关于 $α_2$ 的函数 $W$ 取大值，而又受到方框约束条件的 限制，那么就可以把 $α_2^{new, unclipped}$ 的值 “粘贴” 到 $[L,H]$ 这个间隔内，这样就得到了： </p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note3-img068.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>最终，找到了 $α_2^{new}$ 之后，就可以利用等式 (20) 来代回这个结果，就能得到 $α_1^{new}$ 的最优值。 </p>
<h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><p>（代码还有一点小问题）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># define a struct for storing variables and data</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVMStruct</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_set, labels, C, toler, kernel_option)</span>:</span></div><div class="line">        self.train_x = data_set  <span class="comment"># each row stands for a sample</span></div><div class="line">        self.train_y = labels  <span class="comment"># corresponding label</span></div><div class="line">        self.C = C  <span class="comment"># slack variable</span></div><div class="line">        self.toler = toler  <span class="comment"># termination condition for iteration</span></div><div class="line">        self.num_samples = data_set.shape[<span class="number">0</span>]  <span class="comment"># number of samples</span></div><div class="line">        self.alphas = np.mat(np.zeros((self.num_samples, <span class="number">1</span>)))  <span class="comment"># Lagrange factors for all samples</span></div><div class="line">        self.b = <span class="number">0</span></div><div class="line">        self.error_cache = np.mat(np.zeros((self.num_samples, <span class="number">2</span>)))</div><div class="line">        self.kernel_opt = kernel_option</div><div class="line">        self.kernel_mat = calc_kernel_matrix(self.train_x, self.kernel_opt)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># calculate kernel value</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_kernel_value</span><span class="params">(matrix_x, sample_x, kernel_option)</span>:</span></div><div class="line">    kernel_type = kernel_option[<span class="number">0</span>]</div><div class="line">    num_samples = matrix_x.shape[<span class="number">0</span>]</div><div class="line">    kernel_value = np.mat(np.zeros((num_samples, <span class="number">1</span>)))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> kernel_type == <span class="string">'linear'</span>:</div><div class="line">        kernel_value = matrix_x * sample_x.T</div><div class="line">    <span class="keyword">elif</span> kernel_type == <span class="string">'rbf'</span>:</div><div class="line">        sigma = kernel_option[<span class="number">1</span>]</div><div class="line">        <span class="keyword">if</span> sigma == <span class="number">0</span>:</div><div class="line">            sigma = <span class="number">1.0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_samples):</div><div class="line">            diff = matrix_x[i, :] - sample_x</div><div class="line">            kernel_value[i] = np.exp(diff * diff.T / (<span class="number">-2.0</span> * sigma ** <span class="number">2</span>))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> NameError(<span class="string">'Not support kernel type! You can use linear or rbf!'</span>)</div><div class="line">    <span class="keyword">return</span> kernel_value</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># calculate kernel matrix</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_kernel_matrix</span><span class="params">(train_x, kernel_option)</span>:</span></div><div class="line">    num_samples = train_x.shape[<span class="number">0</span>]</div><div class="line">    kernel_matrix = np.mat(np.zeros((num_samples, num_samples)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_samples):</div><div class="line">        kernel_matrix[:, i] = calc_kernel_value(train_x, train_x[i, :], kernel_option)</div><div class="line">    <span class="keyword">return</span> kernel_matrix</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># calculate the error for alpha k</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_error</span><span class="params">(svm, alpha_k)</span>:</span></div><div class="line">    output_k = float(np.multiply(svm.alphas, svm.train_y).T * svm.kernel_mat[:, alpha_k] + svm.b)</div><div class="line">    error_k = output_k - float(svm.train_y[alpha_k])</div><div class="line">    <span class="keyword">return</span> error_k</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># update the error cache for alpha k after optimize alpha k</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_error</span><span class="params">(svm, alpha_k)</span>:</span></div><div class="line">    error = calc_error(svm, alpha_k)</div><div class="line">    svm.error_cache[alpha_k] = [<span class="number">1</span>, error]</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># select alpha j which has the biggest step</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_alpha_j</span><span class="params">(svm, alpha_i, error_i)</span>:</span></div><div class="line">    svm.error_cache[alpha_i] = [<span class="number">1</span>, error_i]  <span class="comment"># mark as valid(has been optimized)</span></div><div class="line">    candidate_alpha_list = np.nonzero(svm.error_cache[:, <span class="number">0</span>].A)[<span class="number">0</span>]  <span class="comment"># mat.A return array</span></div><div class="line">    max_step = <span class="number">0</span></div><div class="line">    alpha_j = <span class="number">0</span></div><div class="line">    error_j = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># find the alpha with max iterative step</span></div><div class="line">    <span class="keyword">if</span> len(candidate_alpha_list) &gt; <span class="number">1</span>:</div><div class="line">        <span class="keyword">for</span> alpha_k <span class="keyword">in</span> candidate_alpha_list:</div><div class="line">            <span class="keyword">if</span> alpha_k == alpha_i:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            error_k = calc_error(svm, alpha_k)</div><div class="line">            <span class="keyword">if</span> abs(error_k - error_i) &gt; max_step:</div><div class="line">                max_step = abs(error_k - error_i)</div><div class="line">                alpha_j = alpha_k</div><div class="line">                error_j = error_k</div><div class="line">    <span class="comment"># if came in this loop first time, we select alpha j randomly</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        alpha_j = alpha_i</div><div class="line">        <span class="keyword">while</span> alpha_j == alpha_i:</div><div class="line">            alpha_j = int(np.random.uniform(<span class="number">0</span>, svm.num_samples))</div><div class="line">        error_j = calc_error(svm, alpha_j)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> alpha_j, error_j</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># the inner loop for optimizing alpha i and alpha j</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_loop</span><span class="params">(svm, alpha_i)</span>:</span></div><div class="line">    error_i = calc_error(svm, alpha_i)</div><div class="line"></div><div class="line">    <span class="comment"># check and pick up the alpha who violates the KKT condition</span></div><div class="line">    <span class="comment"># satisfy KKT condition</span></div><div class="line">    <span class="comment"># 1) yi*f(i) &gt;= 1 and alpha == 0 (outside the boundary)</span></div><div class="line">    <span class="comment"># 2) yi*f(i) == 1 and 0&lt;alpha&lt; C (on the boundary)</span></div><div class="line">    <span class="comment"># 3) yi*f(i) &lt;= 1 and alpha == C (between the boundary)</span></div><div class="line">    <span class="comment"># violate KKT condition</span></div><div class="line">    <span class="comment"># because y[i]*E_i = y[i]*f(i) - y[i]^2 = y[i]*f(i) - 1, so</span></div><div class="line">    <span class="comment"># 1) if y[i]*E_i &lt; 0, so yi*f(i) &lt; 1, if alpha &lt; C, violate!(alpha = C will be correct)</span></div><div class="line">    <span class="comment"># 2) if y[i]*E_i &gt; 0, so yi*f(i) &gt; 1, if alpha &gt; 0, violate!(alpha = 0 will be correct)</span></div><div class="line">    <span class="comment"># 3) if y[i]*E_i = 0, so yi*f(i) = 1, it is on the boundary, needless optimized</span></div><div class="line">    <span class="keyword">if</span> (svm.train_y[alpha_i] * error_i &lt; -svm.toler) <span class="keyword">and</span> (svm.alphas[alpha_i] &lt; svm.C) <span class="keyword">or</span> \</div><div class="line">                    (svm.train_y[alpha_i] * error_i &gt; svm.toler) <span class="keyword">and</span> (svm.alphas[alpha_i] &gt; <span class="number">0</span>):</div><div class="line"></div><div class="line">        <span class="comment"># step 1: select alpha j</span></div><div class="line">        alpha_j, error_j = select_alpha_j(svm, alpha_i, error_i)</div><div class="line">        alpha_i_old = svm.alphas[alpha_i].copy()</div><div class="line">        alpha_j_old = svm.alphas[alpha_j].copy()</div><div class="line"></div><div class="line">        <span class="comment"># step 2: calculate the boundary L and H for alpha j</span></div><div class="line">        <span class="keyword">if</span> svm.train_y[alpha_i] != svm.train_y[alpha_j]:</div><div class="line">            L = max(<span class="number">0</span>, svm.alphas[alpha_j] - svm.alphas[alpha_i])</div><div class="line">            H = min(svm.C, svm.C + svm.alphas[alpha_j] - svm.alphas[alpha_i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            L = max(<span class="number">0</span>, svm.alphas[alpha_j] + svm.alphas[alpha_i] - svm.C)</div><div class="line">            H = min(svm.C, svm.alphas[alpha_j] + svm.alphas[alpha_i])</div><div class="line">        <span class="keyword">if</span> L == H:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># step 3: calculate eta (the similarity of sample i and j)</span></div><div class="line">        eta = <span class="number">2.0</span> * svm.kernel_mat[alpha_i, alpha_j] - svm.kernel_mat[alpha_i, alpha_i]\</div><div class="line">              - svm.kernel_mat[alpha_j, alpha_j]</div><div class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># step 4: update alpha j</span></div><div class="line">        svm.alphas[alpha_j] -= svm.train_y[alpha_j] * (error_i - error_j) / eta</div><div class="line"></div><div class="line">        <span class="comment"># step 5: clip alpha j</span></div><div class="line">        <span class="keyword">if</span> svm.alphas[alpha_j] &gt; H:</div><div class="line">            svm.alphas[alpha_j] = H</div><div class="line">        <span class="keyword">if</span> svm.alphas[alpha_j] &lt; L:</div><div class="line">            svm.alphas[alpha_j] = L</div><div class="line"></div><div class="line">        <span class="comment"># step 6: if alpha j not moving enough, just return</span></div><div class="line">        <span class="keyword">if</span> abs(alpha_j_old - svm.alphas[alpha_j]) &lt; <span class="number">0.00001</span>:</div><div class="line">            update_error(svm, alpha_j)</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># step 7: update alpha i after optimizing aipha j</span></div><div class="line">        svm.alphas[alpha_i] += svm.train_y[alpha_i] * svm.train_y[alpha_j] \</div><div class="line">                               * (alpha_j_old - svm.alphas[alpha_j])</div><div class="line"></div><div class="line">        <span class="comment"># step 8: update threshold b</span></div><div class="line">        b1 = svm.b - error_i - svm.train_y[alpha_i] * (svm.alphas[alpha_i] - alpha_i_old) \</div><div class="line">                               * svm.kernel_mat[alpha_i, alpha_i] \</div><div class="line">             - svm.train_y[alpha_j] * (svm.alphas[alpha_j] - alpha_j_old) \</div><div class="line">               * svm.kernel_mat[alpha_i, alpha_j]</div><div class="line">        b2 = svm.b - error_j - svm.train_y[alpha_i] * (svm.alphas[alpha_i] - alpha_i_old) \</div><div class="line">                               * svm.kernel_mat[alpha_i, alpha_j] \</div><div class="line">             - svm.train_y[alpha_j] * (svm.alphas[alpha_j] - alpha_j_old) \</div><div class="line">               * svm.kernel_mat[alpha_j, alpha_j]</div><div class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; svm.alphas[alpha_i]) <span class="keyword">and</span> (svm.alphas[alpha_i] &lt; svm.C):</div><div class="line">            svm.b = b1</div><div class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; svm.alphas[alpha_j]) <span class="keyword">and</span> (svm.alphas[alpha_j] &lt; svm.C):</div><div class="line">            svm.b = b2</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            svm.b = (b1 + b2) / <span class="number">2.0</span></div><div class="line"></div><div class="line">        <span class="comment"># step 9: update error cache for alpha i, j after optimize alpha i, j and b</span></div><div class="line">        update_error(svm, alpha_j)</div><div class="line">        update_error(svm, alpha_i)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># the main training procedure</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainSVM</span><span class="params">(train_x, train_y, C, toler, max_iter, kernel_option=<span class="params">(<span class="string">'rbf'</span>, <span class="number">1.0</span>)</span>)</span>:</span></div><div class="line">    <span class="comment"># init data struct for svm</span></div><div class="line">    svm = SVMStruct(np.mat(train_x), np.mat(train_y), C, toler, kernel_option)</div><div class="line"></div><div class="line">    <span class="comment"># start training</span></div><div class="line">    entire_set = <span class="keyword">True</span></div><div class="line">    alpha_pairs_changed = <span class="number">0</span></div><div class="line">    iter_count = <span class="number">0</span></div><div class="line">    <span class="comment"># Iteration termination condition:</span></div><div class="line">    <span class="comment"># 	Condition 1: reach max iteration</span></div><div class="line">    <span class="comment"># 	Condition 2: no alpha changed after going through all samples,</span></div><div class="line">    <span class="comment"># 				 in other words, all alpha (samples) fit KKT condition</span></div><div class="line">    <span class="keyword">while</span> (iter_count &lt; max_iter) <span class="keyword">and</span> ((alpha_pairs_changed &gt; <span class="number">0</span>) <span class="keyword">or</span> entire_set):</div><div class="line">        alpha_pairs_changed = <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># update alphas over all training examples</span></div><div class="line">        <span class="keyword">if</span> entire_set:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(svm.num_samples):</div><div class="line">                alpha_pairs_changed += inner_loop(svm, i)</div><div class="line">            print(<span class="string">'---iter:%d entire set, alpha pairs changed:%d'</span> % (iter_count, alpha_pairs_changed))</div><div class="line"></div><div class="line">            iter_count += <span class="number">1</span></div><div class="line">        <span class="comment"># update alphas over examples where alpha is not 0 &amp; not C (not on boundary)</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            non_bound_alphas_list = np.nonzero((svm.alphas.A &gt; <span class="number">0</span>) * (svm.alphas.A &lt; svm.C))[<span class="number">0</span>]</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> non_bound_alphas_list:</div><div class="line">                alpha_pairs_changed += inner_loop(svm, i)</div><div class="line">            print(<span class="string">'---iter:%d non boundary, alpha pairs changed:%d'</span> % (iter_count, alpha_pairs_changed))</div><div class="line"></div><div class="line">            iter_count += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment"># alternate loop over all examples and non-boundary examples</span></div><div class="line">        <span class="keyword">if</span> entire_set:</div><div class="line">            entire_set = <span class="keyword">False</span></div><div class="line">        <span class="keyword">elif</span> alpha_pairs_changed == <span class="number">0</span>:</div><div class="line">            entire_set = <span class="keyword">True</span></div><div class="line"></div><div class="line">    print(<span class="string">'Congratulations, training complete!'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> svm</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># testing your trained svm model given test set</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">testSVM</span><span class="params">(svm, test_x, test_y)</span>:</span></div><div class="line">    test_x = np.mat(test_x)</div><div class="line">    test_y = np.mat(test_y)</div><div class="line">    num_test_samples = test_x.shape[<span class="number">0</span>]</div><div class="line">    support_vectors_index = np.nonzero(svm.alphas.A &gt; <span class="number">0</span>)[<span class="number">0</span>]</div><div class="line">    support_vectors = svm.train_x[support_vectors_index]</div><div class="line">    support_vector_labels = svm.train_y[support_vectors_index]</div><div class="line">    support_vector_alphas = svm.alphas[support_vectors_index]</div><div class="line">    match_count = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test_samples):</div><div class="line">        kernelValue = calc_kernel_value(support_vectors, test_x[i, :], svm.kernel_opt)</div><div class="line">        predict = kernelValue.T * np.multiply(support_vector_labels, support_vector_alphas) + svm.b</div><div class="line">        <span class="keyword">if</span> np.sign(predict) == np.sign(test_y[i]):</div><div class="line">            match_count += <span class="number">1</span></div><div class="line">    accuracy = float(match_count) / num_test_samples</div><div class="line">    <span class="keyword">return</span> accuracy</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># show your trained svm model only available with 2-D data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showSVM</span><span class="params">(svm)</span>:</span></div><div class="line">    <span class="keyword">if</span> svm.train_x.shape[<span class="number">1</span>] != <span class="number">2</span>:</div><div class="line">        print(<span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>)</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># draw all samples</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(svm.num_samples):</div><div class="line">        <span class="keyword">if</span> svm.train_y[i] == <span class="number">-1</span>:</div><div class="line">            plt.plot(svm.train_x[i, <span class="number">0</span>], svm.train_x[i, <span class="number">1</span>], <span class="string">'ro'</span>)</div><div class="line">        <span class="keyword">elif</span> svm.train_y[i] == <span class="number">1</span>:</div><div class="line">            plt.plot(svm.train_x[i, <span class="number">0</span>], svm.train_x[i, <span class="number">1</span>], <span class="string">'bo'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># mark support vectors</span></div><div class="line">    support_vectors_index = np.nonzero(svm.alphas.A &gt; <span class="number">0</span>)[<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> support_vectors_index:</div><div class="line">        plt.plot(svm.train_x[i, <span class="number">0</span>], svm.train_x[i, <span class="number">1</span>], <span class="string">'yp'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># draw the classify line</span></div><div class="line">    w = np.zeros((<span class="number">2</span>, <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> support_vectors_index:</div><div class="line">        w += np.multiply(svm.alphas[i] * svm.train_y[i], svm.train_x[i, :].T)</div><div class="line">    min_x = min(svm.train_x[:, <span class="number">0</span>])[<span class="number">0</span>, <span class="number">0</span>]</div><div class="line">    max_x = max(svm.train_x[:, <span class="number">0</span>])[<span class="number">0</span>, <span class="number">0</span>]</div><div class="line">    y_min_x = float(-svm.b - w[<span class="number">0</span>] * min_x) / w[<span class="number">1</span>]</div><div class="line">    y_max_x = float(-svm.b - w[<span class="number">0</span>] * max_x) / w[<span class="number">1</span>]</div><div class="line">    plt.plot([min_x, max_x], [y_min_x, y_max_x], <span class="string">'-g'</span>)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    print(<span class="string">"step 1: load data..."</span>)</div><div class="line">    data_set = list()</div><div class="line">    labels = list()</div><div class="line">    file_in = open(<span class="string">'6_test.txt'</span>)</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file_in.readlines():</div><div class="line">        line_arr = line.strip().split(<span class="string">','</span>)</div><div class="line">        data_set.append([float(line_arr[<span class="number">0</span>]), float(line_arr[<span class="number">1</span>])])</div><div class="line">        labels.append(float(line_arr[<span class="number">2</span>]))</div><div class="line"></div><div class="line">    data_set = np.mat(data_set)</div><div class="line">    labels = np.mat(labels).T</div><div class="line">    train_x = data_set[<span class="number">0</span>:<span class="number">81</span>, :]</div><div class="line">    train_y = labels[<span class="number">0</span>:<span class="number">81</span>, :]</div><div class="line">    test_x = data_set[<span class="number">80</span>:<span class="number">101</span>, :]</div><div class="line">    test_y = labels[<span class="number">80</span>:<span class="number">101</span>, :]</div><div class="line"></div><div class="line">    print(<span class="string">"step 2: training..."</span>)</div><div class="line"></div><div class="line">    C = <span class="number">0.6</span></div><div class="line">    toler = <span class="number">0.001</span></div><div class="line">    max_iter = <span class="number">50</span></div><div class="line">    svm_classifier = trainSVM(train_x, train_y, C, toler, max_iter, kernel_option=(<span class="string">'linear'</span>, <span class="number">0</span>))</div><div class="line"></div><div class="line">    print(<span class="string">"step 3: testing..."</span>)</div><div class="line"></div><div class="line">    accuracy = testSVM(svm_classifier, test_x, test_y)</div><div class="line"></div><div class="line">    print(<span class="string">"step 4: show the result..."</span>)</div><div class="line">    print(<span class="string">'The classify accuracy is: %.3f%%'</span> % (accuracy * <span class="number">100</span>))</div><div class="line"></div><div class="line">    showSVM(svm_classifier)</div></pre></td></tr></table></figure></p>
<p>数据集：<br><a href="http://otceoztx6.bkt.clouddn.com/6_test.txt" target="_blank" rel="external">6_test.txt</a><br>运行结果：<br><img src="http://otceoztx6.bkt.clouddn.com/cs229-note0801.png" alt=""></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/公开课笔记/" rel="tag"># 公开课笔记</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/04/cs229-7/" rel="next" title="第七集 最优间隔分类器问题">
                <i class="fa fa-chevron-left"></i> 第七集 最优间隔分类器问题
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/08/cs229-9/" rel="prev" title="第九集 经验风险最小化">
                第九集 经验风险最小化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="ZHANG ZH.Y." />
          <p class="site-author-name" itemprop="name">ZHANG ZH.Y.</p>
           
              <p class="site-description motion-element" itemprop="description">一个<br>为了不做程序员而努力搬砖的<br>代码狗</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#理论知识"><span class="nav-number">1.</span> <span class="nav-text">理论知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#核（Kernels）"><span class="nav-number">1.1.</span> <span class="nav-text">核（Kernels）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化和不可区分的情况（Regularization-And-The-Non-Separable-Case）"><span class="nav-number">1.2.</span> <span class="nav-text">正则化和不可区分的情况（Regularization And The Non-Separable Case）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SMO-优化算法（Sequential-Minimal-Optimization）"><span class="nav-number">1.3.</span> <span class="nav-text">SMO 优化算法（Sequential Minimal Optimization）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#坐标上升算法（Coordinate-Ascent）"><span class="nav-number">1.3.1.</span> <span class="nav-text">坐标上升算法（Coordinate Ascent）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO-优化算法（Sequential-Minimal-Optimization）-1"><span class="nav-number">1.3.2.</span> <span class="nav-text">SMO 优化算法（Sequential Minimal Optimization）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python实现"><span class="nav-number">2.</span> <span class="nav-text">Python实现</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZHANG ZH.Y.</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
