<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="公开课笔记," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="理论知识强化学习和控制（Reinforcement Learning and Control）在监督学习中，我们已经见过的一些算法，输出的标签类 $y$ 都是在训练集中已经存在的。这种情况下，对于每个输入特征 $x$，都有一个对应的标签作为明确的“正确答案”。与之相反，在很多的连续判断和控制的问题中，很难提供这样的明确的显示监督给学习算法。假设我们制作了一个四条腿的机器人，然后要编程让它能走路，而">
<meta name="keywords" content="公开课笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="第十六集 马尔可夫决策过程">
<meta property="og:url" content="http://zzysay.github.io/2017/09/16/cs229-16/index.html">
<meta property="og:site_name" content="ZZY SAY">
<meta property="og:description" content="理论知识强化学习和控制（Reinforcement Learning and Control）在监督学习中，我们已经见过的一些算法，输出的标签类 $y$ 都是在训练集中已经存在的。这种情况下，对于每个输入特征 $x$，都有一个对应的标签作为明确的“正确答案”。与之相反，在很多的连续判断和控制的问题中，很难提供这样的明确的显示监督给学习算法。假设我们制作了一个四条腿的机器人，然后要编程让它能走路，而">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img001.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img002.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img003.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img004.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img005.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img006.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img007.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img008.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img009.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img010.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img011.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img012.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img015.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img016.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img017.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img018.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img019.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img020.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img021.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img022.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img023.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img024.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img025.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img030.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img031.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img032.jpg?imageMogr2/thumbnail/!75p">
<meta property="og:updated_time" content="2017-12-21T10:30:29.569Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第十六集 马尔可夫决策过程">
<meta name="twitter:description" content="理论知识强化学习和控制（Reinforcement Learning and Control）在监督学习中，我们已经见过的一些算法，输出的标签类 $y$ 都是在训练集中已经存在的。这种情况下，对于每个输入特征 $x$，都有一个对应的标签作为明确的“正确答案”。与之相反，在很多的连续判断和控制的问题中，很难提供这样的明确的显示监督给学习算法。假设我们制作了一个四条腿的机器人，然后要编程让它能走路，而">
<meta name="twitter:image" content="http://otceoztx6.bkt.clouddn.com/cs229-note12-img001.jpg?imageMogr2/thumbnail/!75p">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zzysay.github.io/2017/09/16/cs229-16/"/>





  <title>第十六集 马尔可夫决策过程 | ZZY SAY</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZZY SAY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">I AM WHO I AM.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://zzysay.github.io/2017/09/16/cs229-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG ZH.Y.">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZZY SAY">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第十六集 马尔可夫决策过程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-16T18:23:02+08:00">
                2017-09-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/Andrew-Ng公开课/" itemprop="url" rel="index">
                    <span itemprop="name">Andrew Ng公开课</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="理论知识"><a href="#理论知识" class="headerlink" title="理论知识"></a>理论知识</h2><h3 id="强化学习和控制（Reinforcement-Learning-and-Control）"><a href="#强化学习和控制（Reinforcement-Learning-and-Control）" class="headerlink" title="强化学习和控制（Reinforcement Learning and Control）"></a>强化学习和控制（Reinforcement Learning and Control）</h3><p>在监督学习中，我们已经见过的一些算法，输出的标签类 $y$ 都是在训练集中已经存在的。这种情况下，对于每个输入特征 $x$，都有一个对应的标签作为明确的“正确答案”。与之相反，在很多的连续判断和控制的问题中，很难提供这样的明确的显示监督给学习算法。假设我们制作了一个四条腿的机器人，然后要编程让它能走路，而我们并不知道怎么去采取“正确”的动作来进行四条腿的行走，所以就不能给他提供一个明确的监督学习算法来进行模仿。在强化学习的框架下，我们就并不提供监督学习中那种具体的动作方法，而是只给出一个奖励函数，这个函数会告知学习程序什么时候的动作是好的，什么时候的是不好的。在四腿机器人这个样例中，奖励函数会在机器人有进步的时候给出正面回馈，即奖励，而有退步或者摔倒的时候给出负面回馈，可以理解成惩罚。接下来随着时间的退役，学习算法就会解决如何选择正确动作以得到最大奖励。</p>
<a id="more"></a>
<p>强化学习（Reinforcement learning, RL）已经成功用于多种场景了，例如无人直升机的自主飞行，机器人用腿来运动，手机的网络选择，市场营销策略筛选，工厂控制，高效率的网页索引等等。我们对强化学习的探索，要先从马尔可夫决策过程（Markov decision processes, MDP）开始，这个概念给出了强化学习问题的常见形式。</p>
<h3 id="马尔可夫决策过程（Markov-Decision-Processes）"><a href="#马尔可夫决策过程（Markov-Decision-Processes）" class="headerlink" title="马尔可夫决策过程（Markov Decision Processes）"></a>马尔可夫决策过程（Markov Decision Processes）</h3><p>一个马尔可夫决策过程由一个元组$(S, A, {Psa}, γ, R)$组成。其中： </p>
<ol>
<li>$S$ 是一个状态集合。（例如，在无人直升机飞行的案例中，S 就可以是直升机所有的位置和方向的集合）</li>
<li>$A$ 是一个动作结合。（例如，还以无人直升机为例，A 就可以是遥控器上面能够操作的所有动作方向） </li>
<li>$P_{sa}$ 为状态转移概率。对于每个状态 $s ∈ S$ 和动作 $a ∈ A$， $P_{sa}$ 是在状态空间上的一个分布。简单来说，w$P_{sa}$ 给出的是在状态 $s$ 下进行一个动作 $a$ 而要转移到的状态的分布。 </li>
<li>$γ ∈ [0, 1]$ 叫做折扣因子。</li>
<li>$R : S × A → R$ 就是奖励函数。（奖励函数也可以写成仅对状态 $S$ 的函数，这样就可以写成 $R : S → R$）</li>
</ol>
<p>马尔可夫决策过程的动力学过程如下所示：于某个起始状态 $s_0$ 启动，然后选择某个动作 $a_0 ∈ A$ 来执行 $MDP$ 过程。根据所选的动作会有对应的结果，$MDP$ 的状态则转移到某个后继状态，表示为 $s_1$，根据 $s_1 ∼ P_{s_0a_0}$ 得到。然后再选择另外一个动作 $a_1$，接下来又有对应这个动作的状态转移，状态则为 $s_2 ∼ P_{s_1a_1}$。接下来在选择一个动作 $a_2$，就这样进行下去。可以用下面的过程来作为表示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img001.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>通过序列中的所有状态 $s_0, s_1, . . .$  和对应的动作 $a_0, a_1, . . .$，就你能得到给出的总奖励值，即总收益函数为 $R(s_0,a_0) + γR(s_1,a_1) + γ^2R(s_2,a_2) + ···$。如果把奖励函数只作为仅与状态相关的函数，那么这个值就简化成了 $R(s_0) + γR(s_1) + γ^2R(s_2) + ···$。多数情况下，我们都用后面这种仅为状态的函数这种形式，虽然扩展到对应状态-动作两个变量的函数 $R(s,a)$ 也并不难。</p>
<p>强化学习的目标就是找到的一组动作，能使得总收益函数的期望值最大：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img002.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>注意，在时间步长 $t$ 上的奖励函数通过一个参数 $γ^t$ 而进行了缩减。因此，要使得期望最大化，就需要尽可能早积累符号为正的奖励，而尽量推迟负面奖励（惩罚）的出现。在经济方面的应用中，其中的 $R(·)$ 就是盈利金额，$γ$ 也可以理解为利润率的表征，这样有自然的解释，例如今天的一美元就比明天的一美元有更多价值。</p>
<p>有一种策略，是使用任意函数 $π : S → A$，从状态到动作进行映射。如果在状态 $s$，采取动作 $a = π(s)$，就可以说正在执行某种策略 $π$ 。然后还可以针对策略函数 $π$ 来定义一个值函数：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img003.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>$V^π(s)$ 就是从状态 $s$ 开始，根据 $π$ 给出的动作来积累的部分奖励函数的期望总和。</p>
<p>给定一个固定的策略函数 $π$，则对应的值函数 $V^π$ 满足贝尔曼等式（Bellman’s equations）：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img004.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这也就意味着，从状态 $s$ 开始的这个部分奖励的期望总和 $V^π(s)$ 由两部分组成：首先是在状态 $s$ 时候当时立即获得的奖励函数值 $R(s)$，也就是上面式子的第一项；另一个就是第二项，即后续的部分奖励函数值的期望总和。对第二项进行更深入的探索，就能发现这个求和项可以写成 $E_{s′∼P_{sπ(s)}} [V π(s′)]$ 的形式。这种形式也就是从状态 $s′$ 开始的这个部分奖励的期望总和 $V^π(s′)$，此处的 $s′$ 是根据 $P_{sπ}(s)$ 分布的，在 $MDP$ 过程中从状态 $s$ 采取第一个动作 $π(s)$ 之后，确定了这个分布所在的空间。因此，上面的第二项实际上也就是给出了在 $MDP$ 过程中第一步之后的部分奖励的期望总和。</p>
<p>贝尔曼等式可以有效地解出 $V^π$。尤其是在一个有限状态的 $MDP$ 过程中，我们可以把每个状态 $s$ 对应的 $V^π(s)$ 的方程写出来。这样就得到了一系列 $|S|$ 个线性方程，有 $|S|$ 个变量（也就是对应每个状态的未知的 $V^π(s)$ ），这些 $V^π(s)$ 都很容易解出来。<br>然后可以定义出最优值函数。</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img005.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>换一种说法，这个值也就是能用任意一种策略函数来获得的，最佳的可能部分奖励的期望总和。另外对于最优值函数，也有一个版本的贝尔曼等式：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img006.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上面这个等式中的第一项，还是跟之前一样的，还是即时奖励函数值。第二项是在采取了动作 $a$ 之后的所有动作 $a$ 的部分奖励的未来期望总和的最大值。要确保理解这个等式，并且要明白为什么这个等式有意义。</p>
<p>另外还定义了一个策略函数 $π^∗ : S → A$，如下所示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img007.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>注意，这里的 $π^∗(s)$ 给出的动作 $a$ 能够使上面等式（2）当中的 “max” 项取最大值。对于每个状态 $s$ 和每个策略函数 $π$，都有：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img008.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上面的第一个等式关系表明，对应策略函数 $π^∗$ 的值函数 $V^{π^∗}$ 等于对于每个状态 $s$ 的最优值函数 $V^∗$。右边的不等式则表明，$π^∗$ 的值至少也等于任意其他策略函数的值。也就是说，上面在等式（3）当中定义的这个 $π^∗$ 就是最佳策略函数。</p>
<p>注意，这个 $π^∗$ 有一个有趣的特性，它是所有状态 $s$ 下的最佳策略，对于所有的状态 s，都是同样的一个策略函数 $π^∗$ 能够使得等式（1）中的项目取得最大值。这也就意味着无论 MDP 过程的初始状态如何，都可以使用同样的策略函数 $π^∗$。</p>
<h3 id="值迭代和策略迭代（Value-Iteration-And-Policy-Iteration）"><a href="#值迭代和策略迭代（Value-Iteration-And-Policy-Iteration）" class="headerlink" title="值迭代和策略迭代（Value Iteration And Policy Iteration）"></a>值迭代和策略迭代（Value Iteration And Policy Iteration）</h3><p>现在我们要讲两种算法，都能很有效地解决有限状态的马尔可夫决策过程问题。目前为止，我们只考虑有限状态和动作空间的马尔可夫决策过程，也就是状态和动作的个数都是有限的，即 $|S| &lt; ∞, |A| &lt; ∞$。</p>
<p>第一种算法，值迭代（value iteration）：</p>
<ol>
<li>对每个状态 s, 初始化 $V (s) := 0$</li>
<li>重复直到收敛<br>{<br>对每个状态，更新规则<img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img009.jpg?imageMogr2/thumbnail/!75p" alt=""><br>}</li>
</ol>
<p>这个算法可以理解成，利用贝尔曼等式（2）重复更新估计值函数。</p>
<p>在上面的算法的内部循环体中，有两种进行更新的方法。首先，我们可以为每一个状态 $s$ 计算新的值 $V(s)$，然后用新的值覆盖掉所有的旧值。这也叫做同步更新。在这种情况下，此算法可以看做是实现了一个“贝尔曼备份运算符”，这个运算符接收值函数的当前估计，然后映射到一个新的估计值。另外一种方法，就可以使用异步更新。使用这种方法，就可以按照某种次序来遍历所有的状态，然后每次更新其中一个的值。</p>
<p>无论是同步还是异步的更新，都能发现最终值迭代会使 $V$ 收敛到 $V^∗$ 。找到了 $V^∗$ 之后，就可以利用等式（3）来找到最佳策略。</p>
<p>除了值迭代之外，还有另外一种标准算法可以用来在马尔可夫决策过程中寻找一个最佳策略：策略迭代（policy iteration）。算法如下所述：</p>
<ol>
<li>随机初始化 $π$</li>
<li>重复直到收敛<br>{<br>(a) 令 $V := V π$<br>(b) 对每个状态 $s$，令<img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img010.jpg?imageMogr2/thumbnail/!75p" alt=""><br>}</li>
</ol>
<p>因此，在循环体内部就重复计算对于当前策略的值函数，然后使用当前的值函数来更新策略函数。（在步骤 $b$ 中找到的策略 $π$ 也被称为对应 $V$ 的贪心策略）注意，步骤 $a$ 可以通过解贝尔曼等式来实现，在固定策略的情况下，这个等式只是一系列有 $|S|$ 个变量的 $|S|$ 个线性方程。</p>
<p>在上面的算法迭代了某个最大迭代次数之后，$V$ 将会收敛到 $V^∗$，而 $π$ 会收敛到 $π^∗$。</p>
<p>值迭代和策略迭代都是解决马尔可夫决策过程问题的标准算法， 而且目前对于这两个算法哪个更好，还没有一个统一的一致意见。对小规模的 MDPs 来说，策略迭代通常非常快，迭代很少的次数就能瘦脸。然而，对有大规模状态空间的 MDPs，确切求解 $V^π$就要涉及到求解一个非常大的线性方程组系统，可能非常困难。对于这种问题，就可以更倾向于选择值迭代。因此，在实际使用中，值迭代通常比策略迭代更常用。</p>
<h3 id="学习一个马尔可夫决策过程模型（Learning-A-Model-For-An-MDP）"><a href="#学习一个马尔可夫决策过程模型（Learning-A-Model-For-An-MDP）" class="headerlink" title="学习一个马尔可夫决策过程模型（Learning A Model For An MDP）"></a>学习一个马尔可夫决策过程模型（Learning A Model For An MDP）</h3><p>目前为止，我们已经讲了 MDPs，以及用于 MDPs 的一些算法，这都是基于一个假设，即状态转移概率以及奖励函数都是已知的。在很多现实问题中，却未必知道这两样，而是必须从数据中对其进行估计。（通常 $S$，$A$ 和 $γ$ 都是知道的）<br>例如倒立摆问题（inverted pendulum problem），在 MDP 中进行了一系列的试验，过程如下所示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img011.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>其中 $s_i^{(j)}$ 表示的是第 $j$ 次试验中第 $i$ 次的状态，而 $a_i^{(j)}$ 是该状态下的对应动作。在实践中，每个试验都会运行到 MDP 过程停止（例如在倒立摆问题中杆落下），或者会运行到某个大但有限个数的时间步长。<br>有了在 MDP 中一系列试验得到的“经验”，就可以对状态转移概率推导出最大似然估计了：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img012.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>如果上面这个比例出现了 $0/0$ 的情况，对应的情况就是在状态 $s$ 之前没进行过任何动作 $a$，这样就可以简单估计 $P_{sa}(s′)$ 为 $1/|S|$（也就是说把 $P_{sa}$ 估计为在所有状态上的均匀分布）。</p>
<p>注意，如果在 MDP 过程中我们能获得更多经验信息，就能利用新经验来更新估计的状态转移概率，这样很有效率。具体来说，如果我们保存下来等式中的分子和分母的计数，那么观察到更多的试验的时候，就可以很简单地累积这些计数数值。计算这些数值的比例，就能够给出对 $P_{sa}$ 的估计。</p>
<p>利用类似的程序，如果奖励函数 $R$ 未知，我们也可以选择在状态 $s$ 下的期望即时奖励函数 $R(s)$ 来当做是在状态 $s$ 观测到的平均奖励函数。</p>
<p>学习了一个 MDP 模型之后，我们可以使用值迭代或者策略迭代，利用估计的状态转移概率和奖励函数，来去求解这个 MDP 问题。例如，结合模型学习和值迭代，就可以在未知状态转移概率的情况下对 MDP 进行学习，下面就是一种可行的算法：</p>
<ol>
<li>随机初始化 $π$ </li>
<li>重复 {<br>(a) 在 MDP 中执行 $π$ 作为若干次试验<br>(b) 利用上面在 MDP 积累的经验，更新对 $P_{sa}$ 的估计（如果可以的话也对奖励函数 $R$ 进行更新）。<br>(c) 利用估计的状态转移概率和奖励函数，应用值迭代，得到一个新的估计值函数 $V$。<br>(d) 更新 $π$ 为与 $V$ 对应的贪婪策略。<br>}</li>
</ol>
<p>我们注意到，对于这个特定的算法，有一种简单的优化方法，可以让该算法运行得更快。具体来说，在上面算法的内部循环中，使用了值迭代，如果初始化迭代的时候不令 $V = 0$ 启动，而是使用算法中上一次迭代找到的解来初始化，这样就有了一个更好的迭代起点，能让算法更快收敛。</p>
<h3 id="连续状态的马尔可夫决策过程（Continuous-State-MDPs）"><a href="#连续状态的马尔可夫决策过程（Continuous-State-MDPs）" class="headerlink" title="连续状态的马尔可夫决策过程（Continuous State MDPs）"></a>连续状态的马尔可夫决策过程（Continuous State MDPs）</h3><p>目前为止，我们关注的都是有限个状态的马尔可夫决策过程。接下来我们要讲的就是有无限个状态的情况下的算法。例如，对于一辆车，我们可以将其状态表示为 $(x, y, θ, \dot x, \dot y, \dot θ)$ 其中包括位置 $(x, y)$，方向 $θ$，在 x 和 y 方向的速度分量 $\dot x$ 和 $\dot y$ ，以及角速度 $\dot θ$。这样，$S = R^6$ 就是一个无限的状态集合，因为一辆车的位置和方向的个数是有无限可能的。</p>
<h4 id="离散化（Discretization）"><a href="#离散化（Discretization）" class="headerlink" title="离散化（Discretization）"></a>离散化（Discretization）</h4><p>解决连续状态 MDP 问题最简单的方法可能就是将状态空间离散化，然后再使用之前讲过的算法，比如值迭代或者策略迭代来求解。</p>
<p>例如，假设我们有一个二维状态空间 $(s_1，s_2)$，就可以用下面的网格来将这个状态空间离散化：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img015.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>如上图所示，每个网格单元表示的都是一个独立的离散状态 $\overline s$。这样就可以把一个连续状态 MDP 用一个离散状态的 $(\overline S, A, {P_{\overline sa}}, γ, R)$ 来进行逼近，其中的 $\overline S$ 是离散状态集合，而 ${P_{\overline sa}}$ 是此离散状态上的状态转移概率，其他项目同理。然后就可以使用值迭代或者策略迭代来求解出离散状态的 MDP $(\overline S, A, {P_{\overline sa}}, γ, R)$ 和 $π^∗(\overline s)$。当真实系统是某种连续值的状态 $s ∈ S$，而有需要选择某个动作来执行，就可以计算对应的离散化的状态 $\overline s$，然后执行对应的动作 $π^∗(\overline s)$)。</p>
<p>这种离散化方法可以解决很多问题。然而，也有两个缺陷。首先，这种方法使用了对 $V^∗$ 和 $π^∗$ 相当粗糙的表征方法。具体来说，这种方法中假设了在每个离散间隔中的值函数都去一个常数值（也就是说，值函数是在每个网格单元中分段的常数）。<br>要更好理解这样表征的的局限性，可以考虑对下面这一数据集进行函数拟合的监督学习问题：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img016.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>很明显，上面这个数据适合使用线性回归。然而，如果我们对 $x$ 轴进行离散化，那么在每个离散间隔中使用分段常数表示，对同样的数据进行拟合，得到的曲线则如下所示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img017.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这种分段常数表示，对于很多的光滑函数，都不能算好。这会导致输入值缺乏平滑，而且在不同的望各单元中间也没有进行扩展。使用这种表示方法，我们还需要一种非常精细的离散化过程（也就是网格单元要非常小），才能得到一个比较好的近似估计。</p>
<p>第二个缺陷可以称之为维度灾难（curse of dimensionality）。设 $S = R^n$ ，然后我们对每个 $n$ 维度状态离散成 $k$ 个值。这样总共的离散状态的个数就是 $k^n$。在状态空间 $n$ 的维度中，这个值会呈指数级增长，对于大规模问题就不好缩放了。例如，对于一个 $10$ 维的状态，如果我们把每个状态变量离散化成为 $100$ 个值，那么就会有 $100^{10}$ = $10^{20}$ 个离散状态，这个维度太大了，远远超过了当前桌面电脑能应付的能力之外。</p>
<p>根据经验法则，离散化通常非常适合用于 1 维和 2 维的问题（而且有着简单和易于快速实现的优势）。对于 4 维状态的问题，如果使用一点小聪明，仔细挑选离散化方法，有时候效果也不错。如果你超级聪明，并且还得有点幸运，甚至也有可能将离散化方法使用于 6维问题。不过在更高维度的问题中，就更是极其难以使用这种方法了。</p>
<h4 id="值函数近似（Value-Function-Approximation）"><a href="#值函数近似（Value-Function-Approximation）" class="headerlink" title="值函数近似（Value Function Approximation）"></a>值函数近似（Value Function Approximation）</h4><p>现在我们来讲另外一种方法，能用于在连续状态的 MDPs 问题中找出策略，这种方法也就是直接对进行近似 $V^∗$，而不使用离散化。这个方法就叫做值函数近似，在很多强化学习的问题中都有成功的应用。</p>
<h5 id="使用一个模型或模拟器（Using-A-Model-Or-Simulator）"><a href="#使用一个模型或模拟器（Using-A-Model-Or-Simulator）" class="headerlink" title="使用一个模型或模拟器（Using A Model Or Simulator）"></a>使用一个模型或模拟器（Using A Model Or Simulator）</h5><p>要开发一个值函数近似算法，我们要假设已经有一个对于 MDP 的模型，或者模拟器。简单来看，一个模拟器就是一个黑箱子，接收输入的任意状态 $s_t$ 和动作 $a_t$，然后输出下一个状态 $s_{t+1}$，这个新状态是根据状态转移概率 $P_{s_ta_t}$ 取样得来：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img018.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>有很多种方法来获取这样的一个模型，其中一个方法就是使用物理模拟。 例如，倒立摆模拟器就是使用物理定律，给定当前时间 $t$ 和采取的动作 $a$，假设制导系统的所有参数，比如杆的长度、质量等等，来模拟计算在 $t+1$ 时刻杆所处的位置和方向。另外也可以使用现成的物理模拟软件包，这些软件包将一个机械系统的完整物理描述作为输入，当前状态 $s_t$ 和动作 $a_t$，然后计算出未来几分之一秒的系统状态 $s_{t+1}$。</p>
<p>另外一个获取模型的方法，就是从 MDP 中收集的数据来学习生成一个。例如，加入我们在一个 MDP 过程中重复进行了 $m$ 次试验，每一次试验的时间步长为 $T$。这可以用如下方式实现，首先是随机选择动作，然后执行某些特定策略，或者也可以用其他方法选择动作。接下来就能够观测到 $m$ 个状态序列，如下所示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img019.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>然后就可以使用学习算法，作为一个关于 $s_t$ 和 $a_t$ 的函数来预测 $s_{t+1}$。<br>例如，对于线性模型的学习，可以选择下面的形式：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img020.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>然后使用类似线性回归之类的算法。上面的式子中，模型的参数是两个矩阵 $A$ 和 $B$，然后可以使用在 $m$ 次试验中收集的数据来进行估计，选择：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img021.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>通过学习得到 $A$ 和 $B$ 之后，一种选择就是构建一个确定性模型，在此模型中，给定一个输入 $s_t$ 和 $a_t$，输出的则是固定的 $s_{t+1}$。具体来说，也就是根据上面的等式（5）来计算 $s_{t+1}$。</p>
<p>或者用另外一种办法，就是建立一个随机模型，在这个模型中，输出的 $s_{t+1}$ 是关于输入值的一个随机函数，以如下方式建模：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img022.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上面式子中的 $ε_t$ 是噪音项，通常使用一个正态分布来建模，即 $ε_t ∼ N (0, Σ)$。这里，我们把下一个状态 $s_{t+1}$ 写成了当前状态和动作的一个线性函数；不过当然也有非线性函数的可能。比如我们学习一个模型 $s_{t+1} = Aφ_s(s_t) + Bφ_a(a_t)$，其中的 $φ_s$ 和 $φ_a$ 就可以使某些映射了状态和动作的非线性特征。</p>
<p>另外，我们也可以使用非线性的学习算法，例如局部加权线性回归进行学习，来将 $s_{t+1}$ 作为关于 $s_t$ 和 $a_t$ 的函数进行估计。 这些方法也可以用于建立确定性的或者随机的 MDP 模拟器。</p>
<h4 id="拟合值迭代（Fitted-Value-Iteration）"><a href="#拟合值迭代（Fitted-Value-Iteration）" class="headerlink" title="拟合值迭代（Fitted Value Iteration）"></a>拟合值迭代（Fitted Value Iteration）</h4><p>接下来我们要讲的是拟合值迭代算法，作为对一个连续状态 MDP 中值函数的近似。在这部分钟，我们假设学习问题有一个连续的状态空间 $S = R^n$，而动作空间 $A$ 则是小规模的离散空间。</p>
<p>回忆一下值迭代，其中我们使用的更新规则如下所示：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img023.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>在第二节当中，我们把值迭代的更新规则写成了求和的形式：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img024.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>而没有像刚刚上面这样写成在状态上进行积分的形式；这里采用积分的形式来写，是为了表达我们现在面对的是连续状态的情况，而不再是离散状态。<br>拟合值迭代的主要思想就是，在一个有限的状态样本 $s^{(1)}, …  s^{(m)}$ 上，近似执行这一步骤。具体来说，要用一种监督学习算法，比如下面选择的就是线性回归算法，以此来对值函数进行近似，这个值函数可以使关于状态的线性或者非线性函数：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img025.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>上面的式子中，$φ$ 是对状态的某种适当特征映射。对于有限个 $m$ 状态的样本中的每一个状态 $s$，拟合值迭代算法将要首先计算一个量 $y^{(i)}$，这个量可以用 $R(s)+γ \mathop {max}_a E_{s′∼P_{sa}}[V(s′)]$ 来近似。然后使用一个监督学习算法，通过逼近 $R(s)+γ \mathop {max}_a E_{s′∼P_{sa}}[V(s′)]$ 来得到 $V (s)$。具体来说，算法如下所示：</p>
<ol>
<li>从 $S$ 中随机取样 $m$ 个状态 $s^{(1)}, s^{(2)}, . . . s^{(m)} ∈ S$</li>
<li>初始化 $θ := 0$</li>
<li>重复<br>{<br> 对 $i = 1, … , m$ {<pre><code>对每一个动作 $a ∈ A$ {
        取样  $s′^1,... , s′^k ∼ P_{s^{(i)}a}$   (使用一个 MDP 模型).
        设![](http://otceoztx6.bkt.clouddn.com/cs229-note12-img026.jpg?imageMogr2/thumbnail/!66p)
         // 因此， q(a) 是对![](http://otceoztx6.bkt.clouddn.com/cs229-note12-img027.jpg?imageMogr2/thumbnail/!66p)的估计。
        }
设$y^{(i)} = \mathop{max}_a q(a)$
// 因此， $y^{(i)}$ 是对![](http://otceoztx6.bkt.clouddn.com/cs229-note12-img028.jpg?imageMogr2/thumbnail/!66p)的估计
}
// 在原始的值迭代算法是根据 $V (s^{(i)}) = y^{(i)}$ 来对值函数进行更新。而在这里的这个算法中，我们需要的让二者近似相等，即 $V (s^{(i)}) ≈ y^{(i)}$，这可以通过使用监督学习算法（线性回归）来实现。
设![](http://otceoztx6.bkt.clouddn.com/cs229-note12-img029.jpg?imageMogr2/thumbnail/!66p)
</code></pre>}</li>
</ol>
<p>以上，我们就写出了一个拟合值迭代算法，其中使用线性回归作为算法，使 V (s(i)) 逼近 $V (s^{(i)})$。这个步骤完全类似在标准监督学习问题中面对 $m$ 个训练集 $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$ ，而要利用学习得到从 $x$ 到 $y$ 的映射函数的情况；唯一区别无非是这里的 $s$ 扮演了当时 $x$ 的角色。虽然我们上面描述的算法是线性回归，很显然其他的回归算法也都可以使用。</p>
<p>与离散状态集合上进行的的值迭代不同，拟合值迭代并不一定总会收敛。然而，在实践中，通常都还是能收敛的，而且能解决大多数问题。另外还要注意，如果我们使用一个 MDP 的确定性模拟器/模型的话，就可以对拟合值迭代进行简化，设置算法中的 $k = 1$。这是因为等式（7）当中的期望值成为了对确定性分布的期望，所以一个简单样本就足够计算该期望了。否则的话，在上面的算法中，就还要取样出 $k$ 个样本，然后取平均值，来作为对期望值的近似（参考在算法伪代码中的 q(a) 的定义）。</p>
<p>最后，拟合值迭代输出的 $V$，也就是对 $V^∗$ 的一个近似。这同时隐含着对策略函数的定义。 具体来说，当我们的系统处于某个状态 $s$ 的时候，需要选择一个动作，我们可能会选择的动作为：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img030.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这个计算/近似的过程很类似拟合值迭代算法的内部循环体，其中对于每一个动作，我们取样 $s′^1,… , s′^k ∼ P_{s^{(i)}a}$ 来获得近似期望值。（当然，如果模拟器是确定性的，就可以设 $k = 1$）<br>在实际中，通常也有其他方法来实现近似这个步骤。例如，一种很常用的情况就是如果模拟器的形式为 $s_{t+1} = f(s_t,a_t) + ε_t$，其中的 $f$ 是某种关于状态 $s$ 的确定性函数（例如 $f(s_t,a_t) = As_t + Ba_t$），而 $ε$ 是均值为 $0$ 的高斯分布的噪音。在这种情况下，可以通过下面的方法来挑选动作：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img031.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>也就是说，这里只是设置 $ε_t = 0$ ，然后设 $k = 1$。同样地，这也可以通过在等式（8）中使用下面的近似而推出：</p>
<p><img src="http://otceoztx6.bkt.clouddn.com/cs229-note12-img032.jpg?imageMogr2/thumbnail/!75p" alt=""></p>
<p>这里的期望是关于随机分布 $s′ ∼ P_{sa}$ 的。所以只要噪音项目 $ε_t$ 很小，这样的近似通常也是合理的。然而，对于那些不适用于这些近似的问题，就必须使用模型，取样 $k|A|$ 个状态，以便对上面的期望值进行近似，当然这在计算上的开销就很大了。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/公开课笔记/" rel="tag"># 公开课笔记</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/10/cs229-15/" rel="next" title="第十五集 独立成分分析法">
                <i class="fa fa-chevron-left"></i> 第十五集 独立成分分析法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/21/data-mining-apriori/" rel="prev" title="从关联规则到Apriori算法">
                从关联规则到Apriori算法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="ZHANG ZH.Y." />
          <p class="site-author-name" itemprop="name">ZHANG ZH.Y.</p>
           
              <p class="site-description motion-element" itemprop="description">一个<br>为了不做程序员而努力搬砖的<br>代码狗</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#理论知识"><span class="nav-number">1.</span> <span class="nav-text">理论知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习和控制（Reinforcement-Learning-and-Control）"><span class="nav-number">1.1.</span> <span class="nav-text">强化学习和控制（Reinforcement Learning and Control）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#马尔可夫决策过程（Markov-Decision-Processes）"><span class="nav-number">1.2.</span> <span class="nav-text">马尔可夫决策过程（Markov Decision Processes）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#值迭代和策略迭代（Value-Iteration-And-Policy-Iteration）"><span class="nav-number">1.3.</span> <span class="nav-text">值迭代和策略迭代（Value Iteration And Policy Iteration）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习一个马尔可夫决策过程模型（Learning-A-Model-For-An-MDP）"><span class="nav-number">1.4.</span> <span class="nav-text">学习一个马尔可夫决策过程模型（Learning A Model For An MDP）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#连续状态的马尔可夫决策过程（Continuous-State-MDPs）"><span class="nav-number">1.5.</span> <span class="nav-text">连续状态的马尔可夫决策过程（Continuous State MDPs）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#离散化（Discretization）"><span class="nav-number">1.5.1.</span> <span class="nav-text">离散化（Discretization）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#值函数近似（Value-Function-Approximation）"><span class="nav-number">1.5.2.</span> <span class="nav-text">值函数近似（Value Function Approximation）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#使用一个模型或模拟器（Using-A-Model-Or-Simulator）"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">使用一个模型或模拟器（Using A Model Or Simulator）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拟合值迭代（Fitted-Value-Iteration）"><span class="nav-number">1.5.3.</span> <span class="nav-text">拟合值迭代（Fitted Value Iteration）</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZHANG ZH.Y.</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
